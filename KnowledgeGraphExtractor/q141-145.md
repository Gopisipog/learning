# Q141–Q145 — Advanced SQL: CTEs, DML Operations, ACID Properties, and Magic Tables (Expert Guide)

> Expert Learning Guide
> - Audience: Database architects, transaction processing specialists, and advanced SQL developers
> - Prerequisites: Deep understanding of SQL Server internals, transaction management, and trigger mechanisms
> - Objectives:
>   - Master Common Table Expressions for complex hierarchical and analytical queries
>   - Understand DML operation internals and optimization strategies
>   - Implement ACID-compliant transaction designs for high-concurrency systems
>   - Leverage magic tables for advanced auditing and trigger-based solutions
> - How to use: Study transaction mechanics, implement complex CTE patterns, analyze trigger performance

---

## Q141 — Common Table Expressions (CTEs): Advanced Patterns and Optimization

### Theoretical Foundation and Architecture

CTEs represent a powerful SQL construct that provides temporary named result sets within the scope of a single statement. They enable recursive operations, improve query readability, and facilitate complex analytical processing while maintaining optimal execution plans.

#### Advanced CTE Patterns

##### Recursive CTEs for Complex Hierarchies
```sql
-- Advanced organizational hierarchy with path tracking and cycle detection
WITH organizational_hierarchy AS (
    -- Anchor: Root level executives
    SELECT 
        employee_id,
        manager_id,
        employee_name,
        department,
        salary,
        0 as level,
        CAST('/' + CAST(employee_id AS VARCHAR(10)) + '/' AS VARCHAR(1000)) as hierarchy_path,
        CAST(employee_name AS VARCHAR(1000)) as management_chain,
        employee_id as root_manager_id
    FROM employees 
    WHERE manager_id IS NULL
    
    UNION ALL
    
    -- Recursive: Build complete hierarchy
    SELECT 
        e.employee_id,
        e.manager_id,
        e.employee_name,
        e.department,
        e.salary,
        oh.level + 1,
        CAST(oh.hierarchy_path + CAST(e.employee_id AS VARCHAR(10)) + '/' AS VARCHAR(1000)),
        CAST(oh.management_chain + ' -> ' + e.employee_name AS VARCHAR(1000)),
        oh.root_manager_id
    FROM employees e
    INNER JOIN organizational_hierarchy oh ON e.manager_id = oh.employee_id
    WHERE oh.level < 15  -- Prevent infinite recursion
    AND oh.hierarchy_path NOT LIKE '%/' + CAST(e.employee_id AS VARCHAR(10)) + '/%'  -- Cycle detection
),
hierarchy_analytics AS (
    SELECT 
        *,
        COUNT(*) OVER (PARTITION BY root_manager_id) as org_size,
        AVG(salary) OVER (PARTITION BY level) as avg_salary_by_level,
        RANK() OVER (PARTITION BY level ORDER BY salary DESC) as salary_rank_in_level,
        LAG(salary) OVER (PARTITION BY manager_id ORDER BY salary) as peer_comparison
    FROM organizational_hierarchy
)
SELECT 
    employee_id,
    employee_name,
    level,
    management_chain,
    salary,
    avg_salary_by_level,
    salary_rank_in_level,
    org_size,
    CASE 
        WHEN salary > avg_salary_by_level * 1.2 THEN 'Above Average'
        WHEN salary < avg_salary_by_level * 0.8 THEN 'Below Average'
        ELSE 'Average'
    END as salary_category
FROM hierarchy_analytics
ORDER BY root_manager_id, level, salary DESC;
```

##### Analytical CTEs with Window Functions
```sql
-- Advanced sales analytics with multiple CTEs
WITH monthly_sales AS (
    SELECT 
        YEAR(order_date) as sales_year,
        MONTH(order_date) as sales_month,
        customer_id,
        product_category,
        SUM(total_amount) as monthly_revenue,
        COUNT(DISTINCT order_id) as order_count,
        AVG(total_amount) as avg_order_value
    FROM orders o
    JOIN order_items oi ON o.order_id = oi.order_id
    JOIN products p ON oi.product_id = p.product_id
    WHERE order_date >= '2023-01-01'
    GROUP BY YEAR(order_date), MONTH(order_date), customer_id, product_category
),
customer_trends AS (
    SELECT 
        customer_id,
        sales_year,
        sales_month,
        SUM(monthly_revenue) as total_monthly_revenue,
        SUM(order_count) as total_monthly_orders,
        LAG(SUM(monthly_revenue)) OVER (
            PARTITION BY customer_id 
            ORDER BY sales_year, sales_month
        ) as previous_month_revenue,
        LEAD(SUM(monthly_revenue)) OVER (
            PARTITION BY customer_id 
            ORDER BY sales_year, sales_month
        ) as next_month_revenue,
        ROW_NUMBER() OVER (
            PARTITION BY customer_id 
            ORDER BY SUM(monthly_revenue) DESC
        ) as best_month_rank
    FROM monthly_sales
    GROUP BY customer_id, sales_year, sales_month
),
customer_lifecycle AS (
    SELECT 
        customer_id,
        MIN(sales_year * 12 + sales_month) as first_purchase_month,
        MAX(sales_year * 12 + sales_month) as last_purchase_month,
        COUNT(DISTINCT sales_year * 12 + sales_month) as active_months,
        SUM(total_monthly_revenue) as lifetime_value,
        AVG(total_monthly_revenue) as avg_monthly_value,
        CASE 
            WHEN MAX(sales_year * 12 + sales_month) = YEAR(GETDATE()) * 12 + MONTH(GETDATE()) - 1 
            THEN 'Active'
            WHEN MAX(sales_year * 12 + sales_month) >= YEAR(GETDATE()) * 12 + MONTH(GETDATE()) - 3 
            THEN 'Recent'
            ELSE 'Inactive'
        END as customer_status
    FROM customer_trends
    GROUP BY customer_id
)
SELECT 
    cl.customer_id,
    cl.customer_status,
    cl.lifetime_value,
    cl.avg_monthly_value,
    cl.active_months,
    ct.total_monthly_revenue as current_month_revenue,
    ct.previous_month_revenue,
    CASE 
        WHEN ct.previous_month_revenue > 0 
        THEN ((ct.total_monthly_revenue - ct.previous_month_revenue) * 100.0 / ct.previous_month_revenue)
        ELSE NULL
    END as month_over_month_growth,
    RANK() OVER (ORDER BY cl.lifetime_value DESC) as ltv_rank
FROM customer_lifecycle cl
LEFT JOIN customer_trends ct ON cl.customer_id = ct.customer_id
    AND ct.sales_year = YEAR(GETDATE())
    AND ct.sales_month = MONTH(GETDATE()) - 1
ORDER BY cl.lifetime_value DESC;
```

### CTE Performance Optimization

#### Materialization vs. Inline Expansion
```sql
-- Performance comparison: CTE vs Temp Table vs Table Variable
-- CTE approach (may be expanded inline)
WITH sales_summary AS (
    SELECT 
        customer_id,
        SUM(total_amount) as total_sales,
        COUNT(*) as order_count,
        AVG(total_amount) as avg_order_value
    FROM orders
    WHERE order_date >= '2024-01-01'
    GROUP BY customer_id
)
SELECT 
    ss.customer_id,
    c.customer_name,
    ss.total_sales,
    ss.order_count,
    ss.avg_order_value
FROM sales_summary ss
JOIN customers c ON ss.customer_id = c.customer_id
WHERE ss.total_sales > 10000;

-- Temp table approach (always materialized)
CREATE TABLE #sales_summary (
    customer_id INT,
    total_sales DECIMAL(15,2),
    order_count INT,
    avg_order_value DECIMAL(10,2),
    INDEX IX_TempSales_CustomerID (customer_id)
);

INSERT INTO #sales_summary
SELECT 
    customer_id,
    SUM(total_amount),
    COUNT(*),
    AVG(total_amount)
FROM orders
WHERE order_date >= '2024-01-01'
GROUP BY customer_id;

-- Query using temp table
SELECT 
    ss.customer_id,
    c.customer_name,
    ss.total_sales,
    ss.order_count,
    ss.avg_order_value
FROM #sales_summary ss
JOIN customers c ON ss.customer_id = c.customer_id
WHERE ss.total_sales > 10000;

DROP TABLE #sales_summary;
```

---

## Q142 — DELETE, TRUNCATE, and DROP: Advanced DML Operations

### Operation Mechanics and Performance Characteristics

#### Comprehensive DML Operation Analysis
```sql
-- Create test environment for DML operation comparison
CREATE TABLE dml_test_large (
    id INT IDENTITY(1,1) PRIMARY KEY,
    data NVARCHAR(100),
    created_date DATETIME2 DEFAULT SYSDATETIME(),
    INDEX IX_DMLTest_CreatedDate (created_date)
);

CREATE TABLE dml_test_audit (
    operation_type NVARCHAR(20),
    table_name NVARCHAR(50),
    rows_affected INT,
    execution_time_ms INT,
    log_space_used_mb DECIMAL(10,2),
    transaction_log_records INT
);

-- Populate test data
INSERT INTO dml_test_large (data)
SELECT 'Test Data ' + CAST(ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) AS VARCHAR(10))
FROM sys.all_objects a1
CROSS JOIN sys.all_objects a2
WHERE ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) <= 100000;

-- Performance testing procedure
CREATE PROCEDURE sp_TestDMLOperations
AS
BEGIN
    DECLARE @start_time DATETIME2;
    DECLARE @end_time DATETIME2;
    DECLARE @rows_affected INT;
    DECLARE @log_space_before DECIMAL(10,2);
    DECLARE @log_space_after DECIMAL(10,2);
    
    -- Test DELETE operation
    SELECT @log_space_before = (size * 8.0 / 1024) 
    FROM sys.database_files 
    WHERE type = 1; -- Log file
    
    SET @start_time = SYSDATETIME();
    
    DELETE FROM dml_test_large 
    WHERE created_date < DATEADD(minute, -1, SYSDATETIME());
    
    SET @end_time = SYSDATETIME();
    SET @rows_affected = @@ROWCOUNT;
    
    SELECT @log_space_after = (size * 8.0 / 1024) 
    FROM sys.database_files 
    WHERE type = 1;
    
    INSERT INTO dml_test_audit VALUES (
        'DELETE',
        'dml_test_large',
        @rows_affected,
        DATEDIFF(millisecond, @start_time, @end_time),
        @log_space_after - @log_space_before,
        0 -- Would need extended events to capture
    );
    
    -- Repopulate for TRUNCATE test
    INSERT INTO dml_test_large (data)
    SELECT 'Test Data ' + CAST(ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) AS VARCHAR(10))
    FROM sys.all_objects a1
    CROSS JOIN sys.all_objects a2
    WHERE ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) <= 50000;
    
    -- Test TRUNCATE operation
    SELECT @log_space_before = (size * 8.0 / 1024) 
    FROM sys.database_files 
    WHERE type = 1;
    
    SET @start_time = SYSDATETIME();
    
    TRUNCATE TABLE dml_test_large;
    
    SET @end_time = SYSDATETIME();
    
    SELECT @log_space_after = (size * 8.0 / 1024) 
    FROM sys.database_files 
    WHERE type = 1;
    
    INSERT INTO dml_test_audit VALUES (
        'TRUNCATE',
        'dml_test_large',
        50000, -- Approximate
        DATEDIFF(millisecond, @start_time, @end_time),
        @log_space_after - @log_space_before,
        0
    );
    
    -- Test DROP operation
    SET @start_time = SYSDATETIME();
    
    DROP TABLE dml_test_large;
    
    SET @end_time = SYSDATETIME();
    
    INSERT INTO dml_test_audit VALUES (
        'DROP',
        'dml_test_large',
        0,
        DATEDIFF(millisecond, @start_time, @end_time),
        0,
        0
    );
    
    -- Display results
    SELECT * FROM dml_test_audit ORDER BY execution_time_ms;
END;
```

#### Advanced DELETE Patterns
```sql
-- Chunked DELETE for large tables
CREATE PROCEDURE sp_ChunkedDelete
    @table_name NVARCHAR(128),
    @where_clause NVARCHAR(MAX),
    @chunk_size INT = 10000,
    @max_chunks INT = 1000
AS
BEGIN
    DECLARE @sql NVARCHAR(MAX);
    DECLARE @rows_deleted INT = 1;
    DECLARE @total_deleted INT = 0;
    DECLARE @chunk_count INT = 0;
    
    WHILE @rows_deleted > 0 AND @chunk_count < @max_chunks
    BEGIN
        SET @sql = N'
        DELETE TOP (' + CAST(@chunk_size AS VARCHAR(10)) + ') 
        FROM ' + QUOTENAME(@table_name) + '
        WHERE ' + @where_clause;
        
        EXEC sp_executesql @sql;
        SET @rows_deleted = @@ROWCOUNT;
        SET @total_deleted = @total_deleted + @rows_deleted;
        SET @chunk_count = @chunk_count + 1;
        
        -- Brief pause to allow other operations
        IF @rows_deleted > 0
            WAITFOR DELAY '00:00:00.100';
            
        PRINT 'Chunk ' + CAST(@chunk_count AS VARCHAR(10)) + 
              ': Deleted ' + CAST(@rows_deleted AS VARCHAR(10)) + ' rows';
    END
    
    PRINT 'Total deleted: ' + CAST(@total_deleted AS VARCHAR(10)) + ' rows in ' + 
          CAST(@chunk_count AS VARCHAR(10)) + ' chunks';
END;
```

---

## Q143 — Nth Highest Salary: Advanced Ranking and Analytical Queries

### Multiple Approaches and Performance Analysis

#### Comprehensive Ranking Solutions
```sql
-- Method 1: ROW_NUMBER() with CTE (Most efficient)
WITH salary_ranking AS (
    SELECT 
        employee_id,
        employee_name,
        department,
        salary,
        ROW_NUMBER() OVER (ORDER BY salary DESC) as salary_rank,
        DENSE_RANK() OVER (ORDER BY salary DESC) as dense_salary_rank,
        RANK() OVER (ORDER BY salary DESC) as rank_with_gaps
    FROM employees
)
SELECT 
    employee_id,
    employee_name,
    department,
    salary,
    salary_rank
FROM salary_ranking
WHERE salary_rank = 5; -- 5th highest salary

-- Method 2: OFFSET/FETCH (SQL Server 2012+)
SELECT 
    employee_id,
    employee_name,
    salary
FROM employees
ORDER BY salary DESC
OFFSET 4 ROWS  -- Skip first 4 (0-based)
FETCH NEXT 1 ROWS ONLY; -- Get the 5th

-- Method 3: Subquery approach (Less efficient for large datasets)
SELECT 
    employee_id,
    employee_name,
    salary
FROM employees e1
WHERE (
    SELECT COUNT(DISTINCT salary)
    FROM employees e2
    WHERE e2.salary > e1.salary
) = 4; -- 4 salaries higher = 5th highest

-- Method 4: Advanced analytical approach with ties handling
WITH salary_analytics AS (
    SELECT 
        employee_id,
        employee_name,
        department,
        salary,
        DENSE_RANK() OVER (ORDER BY salary DESC) as salary_rank,
        COUNT(*) OVER (PARTITION BY salary) as employees_with_same_salary,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY salary) OVER () as median_salary,
        PERCENT_RANK() OVER (ORDER BY salary) as salary_percentile
    FROM employees
)
SELECT 
    employee_id,
    employee_name,
    department,
    salary,
    salary_rank,
    employees_with_same_salary,
    CASE 
        WHEN employees_with_same_salary > 1 
        THEN 'Tied with ' + CAST(employees_with_same_salary - 1 AS VARCHAR(10)) + ' others'
        ELSE 'Unique salary'
    END as tie_status,
    FORMAT(salary_percentile, 'P2') as salary_percentile_formatted
FROM salary_analytics
WHERE salary_rank = 5;
```

#### Department-wise Nth Highest Salary
```sql
-- Advanced departmental salary analysis
WITH departmental_rankings AS (
    SELECT 
        employee_id,
        employee_name,
        department,
        salary,
        ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as dept_salary_rank,
        DENSE_RANK() OVER (PARTITION BY department ORDER BY salary DESC) as dept_dense_rank,
        COUNT(*) OVER (PARTITION BY department) as dept_employee_count,
        AVG(salary) OVER (PARTITION BY department) as dept_avg_salary,
        MAX(salary) OVER (PARTITION BY department) as dept_max_salary,
        MIN(salary) OVER (PARTITION BY department) as dept_min_salary
    FROM employees
),
nth_highest_by_dept AS (
    SELECT 
        *,
        CASE 
            WHEN dept_salary_rank <= 3 THEN 'Top 3'
            WHEN dept_salary_rank <= dept_employee_count * 0.1 THEN 'Top 10%'
            WHEN dept_salary_rank <= dept_employee_count * 0.25 THEN 'Top 25%'
            ELSE 'Below Top 25%'
        END as salary_tier
    FROM departmental_rankings
)
SELECT 
    department,
    employee_name,
    salary,
    dept_salary_rank,
    salary_tier,
    dept_avg_salary,
    (salary - dept_avg_salary) as salary_vs_dept_avg,
    FORMAT((salary - dept_avg_salary) / dept_avg_salary, 'P2') as variance_percentage
FROM nth_highest_by_dept
WHERE dept_salary_rank = 3 -- 3rd highest in each department
ORDER BY department, salary DESC;
```

---

## Q144 — ACID Properties: Advanced Transaction Management

### ACID Implementation and Concurrency Control

#### Comprehensive ACID Demonstration
```sql
-- Atomicity: All-or-nothing transaction processing
CREATE PROCEDURE sp_TransferFunds
    @from_account INT,
    @to_account INT,
    @amount DECIMAL(15,2)
AS
BEGIN
    SET NOCOUNT ON;
    SET XACT_ABORT ON; -- Ensure atomicity
    
    DECLARE @from_balance DECIMAL(15,2);
    DECLARE @to_balance DECIMAL(15,2);
    
    BEGIN TRANSACTION;
    
    BEGIN TRY
        -- Check source account balance with row-level locking
        SELECT @from_balance = balance
        FROM accounts WITH (UPDLOCK, ROWLOCK)
        WHERE account_id = @from_account;
        
        IF @from_balance IS NULL
            THROW 50001, 'Source account not found', 1;
            
        IF @from_balance < @amount
            THROW 50002, 'Insufficient funds', 1;
        
        -- Verify destination account exists
        SELECT @to_balance = balance
        FROM accounts WITH (UPDLOCK, ROWLOCK)
        WHERE account_id = @to_account;
        
        IF @to_balance IS NULL
            THROW 50003, 'Destination account not found', 1;
        
        -- Perform the transfer (atomic operations)
        UPDATE accounts 
        SET balance = balance - @amount,
            last_modified = SYSDATETIME()
        WHERE account_id = @from_account;
        
        UPDATE accounts 
        SET balance = balance + @amount,
            last_modified = SYSDATETIME()
        WHERE account_id = @to_account;
        
        -- Log the transaction
        INSERT INTO transaction_log (
            from_account, to_account, amount, 
            transaction_type, transaction_date
        )
        VALUES (
            @from_account, @to_account, @amount,
            'TRANSFER', SYSDATETIME()
        );
        
        COMMIT TRANSACTION;
        
        SELECT 'Transfer completed successfully' as Result;
        
    END TRY
    BEGIN CATCH
        IF @@TRANCOUNT > 0
            ROLLBACK TRANSACTION;
            
        DECLARE @error_message NVARCHAR(4000) = ERROR_MESSAGE();
        DECLARE @error_severity INT = ERROR_SEVERITY();
        DECLARE @error_state INT = ERROR_STATE();
        
        THROW @error_severity, @error_message, @error_state;
    END CATCH
END;
```

#### Isolation Level Testing Framework
```sql
-- Comprehensive isolation level testing
CREATE PROCEDURE sp_TestIsolationLevels
AS
BEGIN
    -- Create test table
    CREATE TABLE #isolation_test (
        id INT PRIMARY KEY,
        value INT,
        description NVARCHAR(100)
    );
    
    INSERT INTO #isolation_test VALUES 
    (1, 100, 'Initial Value'),
    (2, 200, 'Second Value'),
    (3, 300, 'Third Value');
    
    -- Test READ UNCOMMITTED (Dirty Reads)
    PRINT 'Testing READ UNCOMMITTED:';
    
    -- Session 1 simulation
    BEGIN TRANSACTION;
    UPDATE #isolation_test SET value = 999 WHERE id = 1;
    
    -- Session 2 simulation (would see dirty read)
    SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;
    SELECT 'READ UNCOMMITTED Result:' as Test, * FROM #isolation_test WHERE id = 1;
    
    ROLLBACK TRANSACTION; -- Session 1 rollback
    
    -- Test READ COMMITTED (Default)
    SET TRANSACTION ISOLATION LEVEL READ COMMITTED;
    SELECT 'READ COMMITTED Result:' as Test, * FROM #isolation_test WHERE id = 1;
    
    -- Test REPEATABLE READ
    SET TRANSACTION ISOLATION LEVEL REPEATABLE READ;
    BEGIN TRANSACTION;
    SELECT 'REPEATABLE READ - First Read:' as Test, * FROM #isolation_test WHERE id = 1;
    
    -- Simulate concurrent update (would be blocked)
    -- UPDATE #isolation_test SET value = 150 WHERE id = 1;
    
    SELECT 'REPEATABLE READ - Second Read:' as Test, * FROM #isolation_test WHERE id = 1;
    COMMIT TRANSACTION;
    
    -- Test SERIALIZABLE
    SET TRANSACTION ISOLATION LEVEL SERIALIZABLE;
    BEGIN TRANSACTION;
    SELECT 'SERIALIZABLE - Range Query:' as Test, COUNT(*) as RecordCount FROM #isolation_test WHERE value BETWEEN 100 AND 300;
    
    -- Phantom read prevention test
    -- INSERT INTO #isolation_test VALUES (4, 250, 'Phantom Record'); -- Would be blocked
    
    SELECT 'SERIALIZABLE - Second Range Query:' as Test, COUNT(*) as RecordCount FROM #isolation_test WHERE value BETWEEN 100 AND 300;
    COMMIT TRANSACTION;
    
    -- Test SNAPSHOT isolation
    IF EXISTS (SELECT 1 FROM sys.databases WHERE name = DB_NAME() AND snapshot_isolation_state = 1)
    BEGIN
        SET TRANSACTION ISOLATION LEVEL SNAPSHOT;
        BEGIN TRANSACTION;
        SELECT 'SNAPSHOT - Consistent Read:' as Test, * FROM #isolation_test;
        COMMIT TRANSACTION;
    END
    
    DROP TABLE #isolation_test;
END;
```

---

## Q145 — Magic Tables: Advanced Trigger Programming

### Magic Table Architecture and Advanced Usage

#### Comprehensive Audit System Using Magic Tables
```sql
-- Advanced audit system with magic tables
CREATE TABLE audit_log (
    audit_id BIGINT IDENTITY(1,1) PRIMARY KEY,
    table_name NVARCHAR(128),
    operation_type NVARCHAR(10),
    primary_key_value NVARCHAR(100),
    column_name NVARCHAR(128),
    old_value NVARCHAR(MAX),
    new_value NVARCHAR(MAX),
    changed_by NVARCHAR(128),
    changed_date DATETIME2,
    session_id INT,
    application_name NVARCHAR(128)
);

-- Advanced trigger using magic tables
CREATE TRIGGER tr_employees_audit
ON employees
AFTER INSERT, UPDATE, DELETE
AS
BEGIN
    SET NOCOUNT ON;
    
    DECLARE @operation_type NVARCHAR(10);
    DECLARE @session_info TABLE (
        session_id INT,
        login_name NVARCHAR(128),
        program_name NVARCHAR(128)
    );
    
    -- Capture session information
    INSERT INTO @session_info
    SELECT @@SPID, SUSER_SNAME(), PROGRAM_NAME();
    
    -- Determine operation type
    IF EXISTS (SELECT 1 FROM inserted) AND EXISTS (SELECT 1 FROM deleted)
        SET @operation_type = 'UPDATE';
    ELSE IF EXISTS (SELECT 1 FROM inserted)
        SET @operation_type = 'INSERT';
    ELSE
        SET @operation_type = 'DELETE';
    
    -- Handle INSERT operations
    IF @operation_type = 'INSERT'
    BEGIN
        INSERT INTO audit_log (
            table_name, operation_type, primary_key_value,
            column_name, old_value, new_value,
            changed_by, changed_date, session_id, application_name
        )
        SELECT 
            'employees',
            'INSERT',
            CAST(i.employee_id AS NVARCHAR(100)),
            'ALL_COLUMNS',
            NULL,
            'employee_id=' + CAST(i.employee_id AS NVARCHAR(50)) + 
            ';employee_name=' + ISNULL(i.employee_name, 'NULL') +
            ';department=' + ISNULL(i.department, 'NULL') +
            ';salary=' + CAST(ISNULL(i.salary, 0) AS NVARCHAR(50)),
            si.login_name,
            SYSDATETIME(),
            si.session_id,
            si.program_name
        FROM inserted i
        CROSS JOIN @session_info si;
    END
    
    -- Handle DELETE operations
    IF @operation_type = 'DELETE'
    BEGIN
        INSERT INTO audit_log (
            table_name, operation_type, primary_key_value,
            column_name, old_value, new_value,
            changed_by, changed_date, session_id, application_name
        )
        SELECT 
            'employees',
            'DELETE',
            CAST(d.employee_id AS NVARCHAR(100)),
            'ALL_COLUMNS',
            'employee_id=' + CAST(d.employee_id AS NVARCHAR(50)) + 
            ';employee_name=' + ISNULL(d.employee_name, 'NULL') +
            ';department=' + ISNULL(d.department, 'NULL') +
            ';salary=' + CAST(ISNULL(d.salary, 0) AS NVARCHAR(50)),
            NULL,
            si.login_name,
            SYSDATETIME(),
            si.session_id,
            si.program_name
        FROM deleted d
        CROSS JOIN @session_info si;
    END
    
    -- Handle UPDATE operations (column-level tracking)
    IF @operation_type = 'UPDATE'
    BEGIN
        -- Track employee_name changes
        INSERT INTO audit_log (
            table_name, operation_type, primary_key_value,
            column_name, old_value, new_value,
            changed_by, changed_date, session_id, application_name
        )
        SELECT 
            'employees', 'UPDATE', CAST(i.employee_id AS NVARCHAR(100)),
            'employee_name', d.employee_name, i.employee_name,
            si.login_name, SYSDATETIME(), si.session_id, si.program_name
        FROM inserted i
        JOIN deleted d ON i.employee_id = d.employee_id
        CROSS JOIN @session_info si
        WHERE ISNULL(i.employee_name, '') != ISNULL(d.employee_name, '');
        
        -- Track salary changes
        INSERT INTO audit_log (
            table_name, operation_type, primary_key_value,
            column_name, old_value, new_value,
            changed_by, changed_date, session_id, application_name
        )
        SELECT 
            'employees', 'UPDATE', CAST(i.employee_id AS NVARCHAR(100)),
            'salary', CAST(d.salary AS NVARCHAR(50)), CAST(i.salary AS NVARCHAR(50)),
            si.login_name, SYSDATETIME(), si.session_id, si.program_name
        FROM inserted i
        JOIN deleted d ON i.employee_id = d.employee_id
        CROSS JOIN @session_info si
        WHERE ISNULL(i.salary, 0) != ISNULL(d.salary, 0);
        
        -- Track department changes
        INSERT INTO audit_log (
            table_name, operation_type, primary_key_value,
            column_name, old_value, new_value,
            changed_by, changed_date, session_id, application_name
        )
        SELECT 
            'employees', 'UPDATE', CAST(i.employee_id AS NVARCHAR(100)),
            'department', d.department, i.department,
            si.login_name, SYSDATETIME(), si.session_id, si.program_name
        FROM inserted i
        JOIN deleted d ON i.employee_id = d.employee_id
        CROSS JOIN @session_info si
        WHERE ISNULL(i.department, '') != ISNULL(d.department, '');
    END
END;
```

#### Advanced Magic Table Patterns
```sql
-- Business rule enforcement using magic tables
CREATE TRIGGER tr_salary_validation
ON employees
AFTER INSERT, UPDATE
AS
BEGIN
    SET NOCOUNT ON;
    
    -- Validate salary increases
    IF UPDATE(salary)
    BEGIN
        DECLARE @invalid_increases TABLE (
            employee_id INT,
            old_salary DECIMAL(15,2),
            new_salary DECIMAL(15,2),
            increase_percentage DECIMAL(5,2)
        );
        
        INSERT INTO @invalid_increases
        SELECT 
            i.employee_id,
            ISNULL(d.salary, 0) as old_salary,
            i.salary as new_salary,
            CASE 
                WHEN ISNULL(d.salary, 0) = 0 THEN 0
                ELSE ((i.salary - d.salary) * 100.0 / d.salary)
            END as increase_percentage
        FROM inserted i
        LEFT JOIN deleted d ON i.employee_id = d.employee_id
        WHERE i.salary > ISNULL(d.salary, 0) * 1.5; -- More than 50% increase
        
        IF EXISTS (SELECT 1 FROM @invalid_increases)
        BEGIN
            DECLARE @error_msg NVARCHAR(1000);
            SELECT @error_msg = 'Salary increase exceeds 50% limit for employee(s): ' +
                STUFF((
                    SELECT ', ' + CAST(employee_id AS VARCHAR(10)) + 
                           ' (' + FORMAT(increase_percentage, 'N2') + '% increase)'
                    FROM @invalid_increases
                    FOR XML PATH('')
                ), 1, 2, '');
            
            THROW 50004, @error_msg, 1;
        END
    END
    
    -- Validate department transfers
    IF UPDATE(department)
    BEGIN
        -- Log department changes for approval workflow
        INSERT INTO department_transfer_requests (
            employee_id,
            old_department,
            new_department,
            request_date,
            status
        )
        SELECT 
            i.employee_id,
            d.department,
            i.department,
            SYSDATETIME(),
            'PENDING_APPROVAL'
        FROM inserted i
        JOIN deleted d ON i.employee_id = d.employee_id
        WHERE ISNULL(i.department, '') != ISNULL(d.department, '');
    END
END;
```

---

## Hands-on Labs
1) Build complex recursive CTE system for organizational hierarchy analysis with performance optimization
2) Implement comprehensive DML operation monitoring and optimization framework
3) Design advanced ACID-compliant transaction system with deadlock detection and resolution
4) Create sophisticated audit system using magic tables with real-time change tracking and business rule enforcement

## Advanced Scenarios
- Design CTE-based analytical queries for complex business intelligence requirements
- Implement high-performance bulk DML operations with minimal logging
- Build distributed transaction coordination systems with ACID guarantees
- Create advanced trigger frameworks for real-time data validation and business rule enforcement

## Performance and Scalability Considerations
- CTE optimization strategies for large datasets and complex hierarchies
- DML operation tuning for high-volume transactional systems
- Transaction isolation optimization for concurrent workloads
- Magic table performance impact analysis and optimization techniques

## Further Reading
- Advanced SQL Server transaction processing and concurrency control
- CTE optimization patterns and execution plan analysis
- Enterprise audit system design and implementation
- Database trigger performance optimization and best practices
